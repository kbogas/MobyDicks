{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: TTT for link prediction\n",
    "### Date: 01/04/2024\n",
    "### Status: Pending\n",
    "### Idea: \n",
    "1. Fit a link prediction method (let's say embeddings) for starters\n",
    "2. Improve the local embeddings using TTT approach based on \"neighborhood\" or \"similar triples\"\n",
    "\n",
    "### Results:\n",
    "Pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intel(R) Extension for Scikit-learn* enabled (https://github.com/intel/scikit-learn-intelex)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will add the inverse train edges as well..\n",
      "Total: 67603 triples in train + eval!)\n",
      "In train: 65776\n",
      "In valid: 1827\n",
      "In test: 1828\n",
      "P@1 Sparsity : 98.55 %\n",
      "P@2 Sparsity : 47.66 %\n",
      "P@3 Sparsity : 4.05 %\n"
     ]
    }
   ],
   "source": [
    "from prime_adj.pam_creation import create_pam_matrices\n",
    "from prime_adj.utils import get_sparsity\n",
    "from prime_adj.data_loading import load_data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearnex import patch_sklearn\n",
    "patch_sklearn()\n",
    "\n",
    "\n",
    "import tqdm\n",
    "\n",
    "project_to_path = {\n",
    "    \"codex-s\": \"/home/kbougatiotis/GIT/Prime_Adj/data/codex-s/\",\n",
    "    \"codex-l\": \"/home/kbougatiotis/GIT/Prime_Adj/data/codex-l/\",\n",
    "    \"WN18RR\": \"/home/kbougatiotis/GIT/Prime_Adj/data/WN18RR/\",\n",
    "    \"FB15k-237\": \"../data/FB15k-237/\",\n",
    "    \"YAGO3-10-DR\": \"../data/YAGO3-10-DR/\",\n",
    "    \"YAGO3-10\": \"../data/YAGO3-10\",\n",
    "    \"NELL-995\": \"../data/NELL-995\",\n",
    "    \"Simpathic\": \"/home/kbougatiotis/GIT/PAM_Biomedical/Simpathic/data/simpathic/stratified_folds_big_with_train/split_0/\"\n",
    "    #\"hetionet\": \"./data/Hetionet/hetionet-v1.0-edges.tsv\",\n",
    "    #\"ogbl-wikikg2\": \"path\",\n",
    "}\n",
    "project_name = 'codex-s'\n",
    "add_inverse_edges=\"YES__INV\"\n",
    "spacing_strategy = 'step_100000'\n",
    "max_order = 3\n",
    "\n",
    "df_train_orig, df_train, df_eval, df_test, already_seen_triples_ = load_data(project_to_path[project_name], project_name, add_inverse_edges=add_inverse_edges)\n",
    "(pam_1hop_lossless_orig,\n",
    "    _,\n",
    "    _,\n",
    "    rel2id_orig,\n",
    "    _) = create_pam_matrices(df_train_orig, max_order=1, use_log=False, eliminate_diagonal=False)\n",
    "\n",
    "(pam_1hop_lossless,\n",
    "    pam_powers,\n",
    "    node2id,\n",
    "    rel2id,\n",
    "    broke_cause_of_sparsity) = create_pam_matrices(df_train, \n",
    "                                                   max_order=max_order, \n",
    "                                                   use_log=True, \n",
    "                                                   eliminate_diagonal=True, \n",
    "                                                   spacing_strategy=spacing_strategy\n",
    "                                                   )\n",
    "for p_i, p in enumerate(pam_powers):\n",
    "    print(f\"P@{p_i+1} Sparsity : {get_sparsity(p):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PPR matrix\n",
      "Hop 1: 98.4968901341888 %\n",
      "Hop 2: 47.66230715012922 %\n",
      "Hop 2: 4.047901505285278 %\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.097065713741522"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import graphblas as gb\n",
    "from prime_adj.utils import get_sparsity\n",
    "\n",
    "def get_ppr_damping_coeff(num_step:int, num_matrix_order:int, a:float=0.85):\n",
    "    b = (1-a)\n",
    "    return (num_step - num_matrix_order + 1) * a ** (num_step - num_matrix_order) * b ** (num_matrix_order)\n",
    "\n",
    "def get_powers_of_adjacency(PPR_W, max_order:int=3):\n",
    "    A_gb = gb.io.from_scipy_sparse(PPR_W)\n",
    "\n",
    "    # Generate the PAM^k matrices\n",
    "    pam_powers = [PPR_W]\n",
    "    pam_power_gb = [A_gb]\n",
    "    for ii in range(1, max_order):\n",
    "        print(f\"Hop {ii}: {get_sparsity(pam_powers[-1])} %\")\n",
    "        cur_previous_power = pam_power_gb[-1].dup()\n",
    "        updated_power_gb = cur_previous_power.mxm(A_gb).new()\n",
    "\n",
    "        updated_power = gb.io.to_scipy_sparse(updated_power_gb)\n",
    "\n",
    "        pam_powers.append(updated_power)\n",
    "        pam_power_gb.append(updated_power_gb)\n",
    "    print(f\"Hop {ii}: {get_sparsity(pam_powers[-1])} %\")\n",
    "    return pam_powers\n",
    "\n",
    "\n",
    "def create_ppr_w(pam, add_inverse_edge):\n",
    "    A_with_ones = pam.copy()\n",
    "    A_with_ones.data = np.ones(len(A_with_ones.data))\n",
    "    eps = 1e-4\n",
    "    tmp_degree = scipy.sparse.diags(1 / (A_with_ones.sum(axis=1) + eps))\n",
    "    PPR_W = scipy.sparse.eye(A_with_ones.shape[0]) + tmp_degree @ A_with_ones\n",
    "    # if add_inverse_edges:\n",
    "    #     rows, cols = PPR_W.nonzero()\n",
    "    #     PPR_W[cols, rows] = PPR_W[rows, cols]\n",
    "    #     PPR_W.eliminate_zeros()\n",
    "    return PPR_W\n",
    "\n",
    "def get_ppr(pam, num_step:int=5, add_inverse_edge:bool=False,  a:float=0.85, fill_diagonal_zeros:bool=True, fill_original_zeros:bool=False):\n",
    "    print(f\"Creating PPR matrix\")\n",
    "    PPR_W = create_ppr_w(pam, add_inverse_edge)\n",
    "    PPR_powers = get_powers_of_adjacency(PPR_W, max_order=num_step)\n",
    "    power_coeffs = [get_ppr_damping_coeff(num_step, i, a) for i in range(num_step)]\n",
    "    aggr = power_coeffs[0] * PPR_powers[0]\n",
    "    for coeff, power in zip(power_coeffs[1:],PPR_powers[1:]):\n",
    "        aggr += coeff*power\n",
    "    if fill_diagonal_zeros:\n",
    "        aggr.setdiag(0)\n",
    "    if fill_original_zeros:\n",
    "        rows, cols = PPR_W.nonzero()\n",
    "        aggr[rows, cols] = 0\n",
    "    aggr.eliminate_zeros()\n",
    "    return aggr\n",
    "\n",
    "ppr = get_ppr(pam_powers[0], num_step=3)\n",
    "get_sparsity(ppr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "\n",
    "# def exp_A(pam_powers, order_of_ppr=3, weights=\"uniform\"):\n",
    "#     aggr = pam_powers[0]\n",
    "\n",
    "#     if isinstance(weights, str):\n",
    "#         if weights == \"uniform\":\n",
    "#             w = np.ones(order_of_ppr)\n",
    "#         elif weights == \"power_decay\":\n",
    "#             w = np.array([1 / np.sqrt(power) for power in range(1, order_of_ppr + 1)])\n",
    "#         elif weights == \"power_incay\":\n",
    "#             w = np.array([np.sqrt(power) for power in range(1, order_of_ppr + 1)])\n",
    "#     else:\n",
    "#         w = weights\n",
    "\n",
    "#     aggr.data = np.ones(len(aggr.data)) * w[0]\n",
    "#     c = 1\n",
    "#     print(w)\n",
    "#     while c < order_of_ppr and c <= len(pam_powers):\n",
    "#         cur_power = pam_powers[c]\n",
    "#         cur_power.data = np.ones(len(cur_power.data)) * w[c]\n",
    "#         aggr = aggr + cur_power\n",
    "#         c += 1\n",
    "#     return aggr\n",
    "\n",
    "\n",
    "\n",
    "from scipy.sparse.linalg import svds\n",
    "# to_use = exp_A(pam_powers, order_of_ppr=max_order, weights='power_incay')\n",
    "u, s, vh = svds(ppr, k=100)\n",
    "# recon = u @ np.diag(s) @ vh\n",
    "# error = ((ppr - recon)**2).sum()\n",
    "# print(f\"Errpr: {error:.2f} Per cell: {error / (ppr.shape[0]*ppr.shape[1]):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['head_mapped'] = df_train['head'].map(node2id)#.astype(int)\n",
    "df_train['tail_mapped'] = df_train['tail'].map(node2id)#.astype(int)\n",
    "df_train['rel_mapped'] = df_train['rel'].map(rel2id)\n",
    "df_train = df_train.dropna()\n",
    "df_train['head_mapped'] = df_train['head_mapped'].astype(int)\n",
    "df_train['tail_mapped'] = df_train['tail_mapped'].astype(int)\n",
    "\n",
    "df_eval['head_mapped'] = df_eval['head'].map(node2id)#.astype(int)\n",
    "df_eval['tail_mapped'] = df_eval['tail'].map(node2id)#.astype(int)\n",
    "df_eval['rel_mapped'] = df_eval['rel'].map(rel2id)\n",
    "df_eval = df_eval.dropna()\n",
    "df_eval['head_mapped'] = df_eval['head_mapped'].astype(int)\n",
    "df_eval['tail_mapped'] = df_eval['tail_mapped'].astype(int)\n",
    "\n",
    "df_test['head_mapped'] = df_test['head'].map(node2id)#.astype(int)\n",
    "df_test['rel_mapped'] = df_test['rel'].map(rel2id)#.astype(int)\n",
    "\n",
    "df_test['tail_mapped'] = df_test['tail'].map(node2id)#.astype(int)\n",
    "df_test = df_test.dropna()\n",
    "df_test['head_mapped'] = df_test['head_mapped'].astype(int)\n",
    "df_test['tail_mapped'] = df_test['tail_mapped'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ppr = u @ vh\n",
    "# argsorted = np.fliplr(np.argsort(ppr, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_mapped = df_test.copy()\n",
    "df_test_mapped[\"rel\"] = df_test[\"rel\"].map(rel2id)\n",
    "df_test_mapped[\"head\"] = df_test[\"head\"].map(node2id)\n",
    "df_test_mapped[\"tail\"] = df_test[\"tail\"].map(node2id)\n",
    "df_test_mapped.dropna(inplace=True)\n",
    "\n",
    "node2uniqe = {node_index:unq_index for unq_index, node_index in enumerate(df_test_mapped[\"head\"].unique().astype(int))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1453079/1317532526.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for head_id in tqdm.tqdm_notebook(node2uniqe.keys()):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dde1eda60b341c2ace5cb7a0216cba3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1045 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "argsorted = []\n",
    "for head_id in tqdm.tqdm_notebook(node2uniqe.keys()):\n",
    "    try:\n",
    "        ppr.nnz\n",
    "        cur_sort = np.argsort(ppr[[head_id], :].toarray().ravel())[::-1]\n",
    "    except:\n",
    "        cur_sort = np.argsort(ppr[head_id].ravel())[::-1]\n",
    "    argsorted.append(cur_sort)\n",
    "argsorted = np.array(argsorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2034x2034 sparse array of type '<class 'numpy.int64'>'\n",
       "\twith 32611 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pam_1hop_lossless_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.83138103,  4.94985556,  5.00027938,  5.09473394,  5.20881341,\n",
       "        5.27151821,  5.40173932,  5.49074566,  5.68810948, 49.95493098])"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse.linalg import svds as sp_svds\n",
    "u_lossless, s_lossless, vh_lossless = sp_svds(np.random.rand(100,100), k=10, which='LM')\n",
    "s_lossless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  261077.13205047,   267638.977796  ,   273114.4676951 ,\n",
       "         339503.09730677,   340592.46022521,   343739.38725787,\n",
       "         375431.38315829,   413215.76656574,   483089.28053417,\n",
       "       21040297.18205072])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppr_lossless = u_lossless @ np.diag(s_lossless) @ vh_lossless\n",
    "# ppr_lossless = np.clip(ppr_lossless, 0, pam_1hop_lossless_orig.max())\n",
    "# possible_labels = np.sort(np.unique([0] + pam_1hop_lossless_orig.data.tolist()))\n",
    "# inds = np.digitize(ppr_lossless, possible_labels) - 1\n",
    "# ppr_lossless = possible_labels[inds]\n",
    "argsorted_lossless = np.fliplr(np.argsort(ppr_lossless, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004755627649672863"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(ppr_lossless - pam_1hop_lossless_orig) / np.prod(pam_1hop_lossless_orig.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['head', 'rel', 'tail', 'rel_mapped', 'head_mapped', 'tail_mapped'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1453079/2384468936.py:7: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for triple in tqdm.tqdm_notebook(all_triples.to_records()):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fd1cfccfd4f43288fb93611034b936a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tqdm\n",
    "from collections import defaultdict\n",
    "def get_filtering_cache(df_train, df_eval, df_test):\n",
    "    cache_triples = defaultdict(list)\n",
    "    all_triples = pd.concat((df_train, df_eval, df_test))\n",
    "    print(all_triples.columns)\n",
    "    for triple in tqdm.tqdm_notebook(all_triples.to_records()):\n",
    "        # Adding h,r ->t\n",
    "        cache_triples[(triple[1], triple[2])].append(triple[3])\n",
    "        # Addubg r, t -> h\n",
    "        cache_triples[(triple[2], triple[3])].append(triple[1])\n",
    "    return cache_triples\n",
    "\n",
    "id2node = {v:k for k,v in node2id.items()}\n",
    "id2rel_orig = {v:k for k,v in rel2id_orig.items()}\n",
    "\n",
    "\n",
    "cache_triples = get_filtering_cache(df_train_orig, df_eval, df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = []\n",
    "# for i_row, row in tqdm.tqdm_notebook(df_test_mapped.iterrows(), total=len(df_test_mapped)):\n",
    "#     unq_index = node2uniqe[int(row['head'])]\n",
    "#     sim_scores = np.abs(ppr_lossless[unq_index] - row['rel_mapped'])\n",
    "#     already_seen_tails = cache_triples[(row['head'], row['rel'])]\n",
    "#     if len(already_seen_tails) > 0:\n",
    "#         sim_scores[np.array([node2id[tail] for tail in already_seen_tails])] =  max(sim_scores) + 1 \n",
    "#     tail_indices = np.argsort(sim_scores)\n",
    "#     index_of_tail = int(row['tail'])\n",
    "#     rank = tail_indices.tolist().index(index_of_tail) + 1\n",
    "#     cur_res = {\n",
    "#         \"predicted\": tail_indices,\n",
    "#         \"probas\": sim_scores[tail_indices],\n",
    "#         \"rank\": rank,\n",
    "#         **row,\n",
    "#     }\n",
    "#     res.append(cur_res) \n",
    "# df_res_ppr = pd.DataFrame(res)\n",
    "\n",
    "# pr_results = {}\n",
    "# pr_results[\"MRR\"] = (1 / df_res_ppr[\"rank\"]).mean()\n",
    "# print(f\"MRR:{pr_results['MRR']:.4f}\")\n",
    "\n",
    "# for k in [1, 3, 10,100,1000,10000,20000] + [0.1, 0.2, 0.5]:\n",
    "#     if isinstance(k, int):\n",
    "#         k_int = k\n",
    "#     else:\n",
    "#         k_int = int(k*argsorted.shape[1])\n",
    "#     pr_results[f\"h@{k}\"] = (df_res_ppr[\"rank\"] <= k_int).sum() / df_res_ppr.shape[0]\n",
    "#     print(f\"Hits@{k}: {pr_results[f'h@{k}']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1453079/4041784970.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for i_row, row in tqdm.tqdm_notebook(df_test_mapped.iterrows(), total=len(df_test_mapped)):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e93d96fbd7044d07994d88eb48d7ddd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1828 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR:0.0070\n",
      "Hits@1: 0.0005\n",
      "Hits@3: 0.0038\n",
      "Hits@10: 0.0137\n",
      "Hits@100: 0.1012\n",
      "Hits@1000: 0.4136\n",
      "Hits@10000: 1.0000\n",
      "Hits@20000: 1.0000\n",
      "Hits@0.1: 0.1887\n",
      "Hits@0.2: 0.3206\n",
      "Hits@0.5: 0.4152\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for i_row, row in tqdm.tqdm_notebook(df_test_mapped.iterrows(), total=len(df_test_mapped)):\n",
    "    unq_index = node2uniqe[int(row['head'])]\n",
    "    tail_indices = argsorted_lossless[unq_index]\n",
    "    index_of_tail = int(row['tail'])\n",
    "    rank = tail_indices.tolist().index(index_of_tail) + 1\n",
    "    cur_res = {\n",
    "        \"predicted\": tail_indices,\n",
    "        \"probas\": argsorted_lossless[unq_index][tail_indices],\n",
    "        \"rank\": rank,\n",
    "        **row,\n",
    "    }\n",
    "    res.append(cur_res) \n",
    "df_res_ppr = pd.DataFrame(res)\n",
    "\n",
    "pr_results = {}\n",
    "pr_results[\"MRR\"] = (1 / df_res_ppr[\"rank\"]).mean()\n",
    "print(f\"MRR:{pr_results['MRR']:.4f}\")\n",
    "\n",
    "for k in [1, 3, 10,100,1000,10000,20000] + [0.1, 0.2, 0.5]:\n",
    "    if isinstance(k, int):\n",
    "        k_int = k\n",
    "    else:\n",
    "        k_int = int(k*argsorted.shape[1])\n",
    "    pr_results[f\"h@{k}\"] = (df_res_ppr[\"rank\"] <= k_int).sum() / df_res_ppr.shape[0]\n",
    "    print(f\"Hits@{k}: {pr_results[f'h@{k}']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementing a two step approach.\n",
    "\n",
    "# Given a query, fetch similar queries + "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will add the inverse train edges as well..\n",
      "Total: 67603 triples in train + eval!)\n",
      "In train: 65776\n",
      "In valid: 1827\n",
      "In test: 1828\n",
      "2034 2034 Graph(num_nodes=4068, num_edges=69431,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={'rel': Scheme(shape=(), dtype=torch.int64)})\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot unpack non-iterable NoneType object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[266], line 209\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmean(auc_scores)\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Main Execution\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m train_graph, val_graph, test_graph, num_entities, num_relations, node2id_graph, rel2id_graph \u001b[38;5;241m=\u001b[39m load_wn18rr_true(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWN18RR\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;66;03m#load_wn18rr_sample()\u001b[39;00m\n\u001b[1;32m    210\u001b[0m model \u001b[38;5;241m=\u001b[39m TransE(num_entities, num_relations, embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m    211\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import dgl\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Sample WN18RR data (replace with actual dataset loading)\n",
    "def load_wn18rr_sample(num_entities=100, num_relations=10, num_edges=500):\n",
    "    src = np.random.randint(0, num_entities, num_edges)\n",
    "    dst = np.random.randint(0, num_entities, num_edges)\n",
    "    rel = np.random.randint(0, num_relations, num_edges)\n",
    "    graph = dgl.graph((src, dst))\n",
    "    graph.edata['rel'] = torch.tensor(rel)\n",
    "\n",
    "    train_mask = torch.rand(num_edges) < 0.8\n",
    "    val_mask = (torch.rand(num_edges) < 0.1) & ~train_mask\n",
    "    test_mask = ~(train_mask | val_mask)\n",
    "\n",
    "    train_graph = dgl.edge_subgraph(graph, train_mask, relabel_nodes=False)\n",
    "    val_graph = dgl.edge_subgraph(graph, val_mask, relabel_nodes=False)\n",
    "    test_graph = dgl.edge_subgraph(graph, test_mask, relabel_nodes=False)\n",
    "\n",
    "    return train_graph, val_graph, test_graph, num_entities, num_relations, {i:i for i in np.arange(num_entities)}, {i:i for i in np.arange(num_relations)}\n",
    "\n",
    "\n",
    "# Sample WN18RR data (replace with actual dataset loading)\n",
    "def load_wn18rr_true(name_of_project):\n",
    "    df_train_orig, df_train, df_eval, df_test, already_seen_triples_ = load_data(project_to_path[project_name], project_name, add_inverse_edges=add_inverse_edges)\n",
    "    df_train['type']= 'train'\n",
    "    df_eval['type'] = 'eval'\n",
    "    df_test['type'] = 'test'\n",
    "    \n",
    "    df_all = pd.concat([df_train, df_eval, df_test])\n",
    "    unq_nodes = np.concatenate((df_all['head'].unique(), df_all['tail'].unique()))\n",
    "    unq_rels = df_all['rel'].unique()\n",
    "    node2id_graph = {v:index for index, v in enumerate(unq_nodes)}\n",
    "    rel2id_graph = {r:index for index, r in enumerate(unq_rels)}\n",
    "    \n",
    "    df_all['head_mapped'] = df_all['head'].map(node2id_graph)\n",
    "    df_all['tail_mapped'] = df_all['tail'].map(node2id_graph)\n",
    "    df_all['rel_mapped'] = df_all['rel'].map(rel2id_graph)\n",
    "    df_all.dropna(inplace=True)\n",
    "    \n",
    "    graph = dgl.graph((df_all['head_mapped'].values.astype(int), df_all['tail_mapped'].values.astype(int)))\n",
    "    graph.edata['rel'] = torch.tensor(df_all['rel_mapped'].values)\n",
    "    print(df_all['head_mapped'].unique().shape[0], df_all['tail_mapped'].unique().shape[0], graph)\n",
    "    return \n",
    "    train_mask = torch.tensor((df_all['type'] == 'train').values.astype(bool))\n",
    "    val_mask = torch.tensor((df_all['type'] == 'eval').values.astype(bool))\n",
    "    test_mask = torch.tensor((df_all['type'] == 'test').values.astype(bool))\n",
    "\n",
    "    train_graph = dgl.edge_subgraph(graph, train_mask, relabel_nodes=False)\n",
    "    val_graph = dgl.edge_subgraph(graph, val_mask, relabel_nodes=False)\n",
    "    test_graph = dgl.edge_subgraph(graph, test_mask, relabel_nodes=False)\n",
    "\n",
    "    return train_graph, val_graph, test_graph, len(node2id), len(rel2id), node2id_graph, rel2id_graph\n",
    "\n",
    "class TransE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim, margin=1.0, p_norm=1):\n",
    "        super(TransE, self).__init__()\n",
    "        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "        self.margin = margin\n",
    "        self.p_norm = p_norm\n",
    "\n",
    "        nn.init.xavier_uniform_(self.entity_embeddings.weight.data)\n",
    "        nn.init.xavier_uniform_(self.relation_embeddings.weight.data)\n",
    "\n",
    "    def forward(self, h, r, t):\n",
    "        print(h,h.dtype)\n",
    "        print(r,r.dtype)\n",
    "        print(t,t.dtype)\n",
    "        h_emb = self.entity_embeddings(h)\n",
    "        r_emb = self.relation_embeddings(r)\n",
    "        t_emb = self.entity_embeddings(t)\n",
    "        return h_emb, r_emb, t_emb\n",
    "\n",
    "    def compute_loss(self, h_emb, r_emb, t_emb, neg_h_emb, neg_t_emb):\n",
    "        pos_score = torch.norm(h_emb + r_emb - t_emb, p=self.p_norm, dim=-1)\n",
    "        # print(neg_h_emb.shape, r_emb.shape, t_emb.shape)\n",
    "        # print(r_emb.reshape(r_emb.shape[0], 1, r_emb.shape[1]).repeat(1,10,1).shape, t_emb.reshape(t_emb.shape[0], 1, t_emb.shape[1]).repeat(1,10,1).shape)\n",
    "        expanded_r_emb =  r_emb.reshape(r_emb.shape[0], 1, r_emb.shape[1]).repeat(1,10,1)\n",
    "        expanded_h_emb = h_emb.reshape(h_emb.shape[0], 1, h_emb.shape[1]).repeat(1,10,1)\n",
    "        expanded_t_emb = t_emb.reshape(t_emb.shape[0], 1, t_emb.shape[1]).repeat(1,10,1)\n",
    "        neg_score_h = torch.norm(neg_h_emb + expanded_r_emb -expanded_t_emb, p=self.p_norm, dim=-1)\n",
    "        neg_score_t = torch.norm(expanded_h_emb + expanded_r_emb - neg_t_emb, p=self.p_norm, dim=-1)\n",
    "\n",
    "        loss_h = torch.relu(self.margin + pos_score - neg_score_h.mean(1)).mean()\n",
    "        loss_t = torch.relu(self.margin + pos_score - neg_score_t.mean(1)).mean()\n",
    "        return loss_h + loss_t\n",
    "\n",
    "    def get_embeddings(self, h, r, t):\n",
    "        h_emb = self.entity_embeddings(h) if h is not None else None\n",
    "        r_emb = self.relation_embeddings(r) if r is not None else None\n",
    "        t_emb = self.entity_embeddings(t) if t is not None else None\n",
    "        return h_emb, r_emb, t_emb\n",
    "\n",
    "def generate_negative_samples(graph, batch_size):\n",
    "    num_neg = 10 # number of negative samples per positive sample\n",
    "    neg_head = torch.randint(0, graph.num_nodes(), (batch_size,num_neg))\n",
    "    neg_tail = torch.randint(0, graph.num_nodes(), (batch_size,num_neg))\n",
    "    return neg_head, neg_tail\n",
    "\n",
    "def train(model, graph, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        h, t = graph.edges()\n",
    "        r = graph.edata['rel']\n",
    "        h_emb, r_emb, t_emb = model(h, r, t)\n",
    "        neg_h, neg_t = generate_negative_samples(graph, h_emb.shape[0])\n",
    "        print(h_emb.shape, r_emb.shape, neg_h.shape, neg_t.shape)\n",
    "        neg_h_emb = model.entity_embeddings(neg_h)\n",
    "        neg_t_emb = model.entity_embeddings(neg_t)\n",
    "\n",
    "        loss = model.compute_loss(h_emb, r_emb, t_emb, neg_h_emb, neg_t_emb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# def extract_neighborhood(graph, node, k=2):\n",
    "#     # bfs = dgl.sampling.bfs_nodes_generator(graph, node, k)\n",
    "#     # neighborhood_nodes = torch.cat([nodes for nodes in bfs])\n",
    "#     # subgraph = dgl.node_subgraph(graph, neighborhood_nodes)\n",
    "#     print(node.item())\n",
    "#     subgraph, _ = dgl.khop_in_subgraph(graph, nodes=node.item(), k=k, relabel_nodes=False)\n",
    "#     return subgraph\n",
    "\n",
    "\n",
    "def extract_neighborhood(graph, node, k=2):\n",
    "    \"\"\"Extracts the k-hop in-neighborhood for a node in a homogeneous graph.\"\"\"\n",
    "    visited = {node.item()}\n",
    "    queue = [(node.item(), 0)]  # (node, distance)\n",
    "    nodes_to_include = [node.item()]\n",
    "\n",
    "    while queue:\n",
    "        current_node, distance = queue.pop(0)\n",
    "        if distance >= k:\n",
    "            continue\n",
    "\n",
    "        neighbors = graph.in_edges(current_node)[0].tolist() # get source nodes of in_edges\n",
    "        for neighbor in neighbors:\n",
    "            if neighbor not in visited:\n",
    "                visited.add(neighbor)\n",
    "                queue.append((neighbor, distance + 1))\n",
    "                nodes_to_include.append(neighbor)\n",
    "\n",
    "    subgraph = dgl.node_subgraph(graph, nodes_to_include, relabel_nodes=False, store_ids=True)\n",
    "    return subgraph\n",
    "\n",
    "def adapt_transE(model, query, neighborhood, learning_rate=0.01, num_steps=10):\n",
    "    h_test, r_test, t_test = query\n",
    "    h_test_emb, r_test_emb, t_test_emb = model.get_embeddings(h_test, r_test, t_test)\n",
    "\n",
    "    if t_test is None:\n",
    "        adapted_h = h_test_emb.clone().detach().requires_grad_(True)\n",
    "        adapted_t = model.entity_embeddings.weight.clone().detach().requires_grad_(True)\n",
    "    elif h_test is None:\n",
    "        adapted_h = model.entity_embeddings.weight.clone().detach().requires_grad_(True)\n",
    "        adapted_t = t_test_emb.clone().detach().requires_grad_(True)\n",
    "\n",
    "    optimizer = optim.Adam([adapted_h, adapted_t], lr=learning_rate)\n",
    "\n",
    "    for _ in range(num_steps):\n",
    "        optimizer.zero_grad()\n",
    "        loss = torch.tensor(0.0, requires_grad=True)\n",
    "        for h_i, t_i, eid in zip(neighborhood.edges()[0], neighborhood.edges()[1], neighborhood.edata[dgl.EID]):\n",
    "            print(h_i, h_i.shape, t_i, t_i.shape, eid, eid.shape)\n",
    "            if eid > train_graph.edata['rel'].shape[0]:\n",
    "                print('Wrong indexing...')\n",
    "            else:\n",
    "                r_i = train_graph.edata['rel'][eid]\n",
    "                h_i_emb, r_i_emb, t_i_emb = model.get_embeddings(h_i, r_i, t_i)\n",
    "                if t_test is None:\n",
    "                    loss += torch.relu(model.margin + torch.norm(adapted_h + r_i_emb - t_i_emb,p=model.p_norm) - torch.norm(h_i_emb + r_i_emb - t_i_emb, p=model.p_norm))\n",
    "                    print('added loss', loss)\n",
    "                elif h_test is None:\n",
    "                    loss += torch.relu(model.margin + torch.norm(h_i_emb + r_i_emb - adapted_t, p=model.p_norm) - torch.norm(h_i_emb + r_i_emb - t_i_emb, p=model.p_norm))\n",
    "                    print('added loss', loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if t_test is None:\n",
    "        scores = -torch.norm(adapted_h + r_test_emb - model.entity_embeddings.weight, dim=1, p=model.p_norm)\n",
    "        return scores\n",
    "    elif h_test is None:\n",
    "        scores = -torch.norm(model.entity_embeddings.weight + r_test_emb - adapted_t, dim=1, p=model.p_norm)\n",
    "        return scores\n",
    "\n",
    "def evaluate(model, test_graph):\n",
    "    model.eval()\n",
    "    auc_scores = []\n",
    "    with torch.no_grad():\n",
    "        for h, t, eid in zip(test_graph.edges()[0], test_graph.edges()[1], test_graph.edata[dgl.EID]):\n",
    "            if eid > test_graph.edata['rel'].shape[0]:\n",
    "                print('Wrong indexing...')\n",
    "            else:\n",
    "                r = test_graph.edata['rel'][eid]\n",
    "                neighborhood = extract_neighborhood(test_graph, h)\n",
    "                scores = adapt_transE(model, (h, r, None), neighborhood)\n",
    "                labels = torch.zeros(model.entity_embeddings.num_embeddings)\n",
    "                labels[t] = 1.0\n",
    "                auc = roc_auc_score(labels.numpy(), scores.numpy())\n",
    "                auc_scores.append(auc)\n",
    "    return np.mean(auc_scores)\n",
    "\n",
    "# Main Execution\n",
    "train_graph, val_graph, test_graph, num_entities, num_relations, node2id_graph, rel2id_graph = load_wn18rr_true('WN18RR')#load_wn18rr_sample()\n",
    "model = TransE(num_entities, num_relations, embedding_dim=50)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "train(model, train_graph, optimizer, epochs=30)\n",
    "auc = evaluate(model, test_graph)\n",
    "print(f\"Test AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train, df_eval, df_test])\n",
    "graph2 = dgl.graph((df_all['head_mapped'].values.astype(int), df_all['tail_mapped'].values.astype(int)))\n",
    "graph2.edata['rel'] = torch.tensor(df_all['rel_mapped'].values)\n",
    "graph2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_train, df_eval, df_test])\n",
    "df_all['rel_mapped_int'] = df_all['rel'].map(rel2id_graph) + 1\n",
    "graph2 = dgl.graph((df_all['head_mapped'].values.astype(int), df_all['tail_mapped'].values.astype(int)))\n",
    "\n",
    "graph2.edata['rel'] = torch.tensor(df_all['rel_mapped_int'].values.astype(int))\n",
    "\n",
    "\n",
    "train_mask = torch.zeros(len(df_all), dtype=int)\n",
    "train_mask[:len(df_train)] = 1\n",
    "val_mask = torch.zeros(len(df_all), dtype=int)\n",
    "val_mask[len(df_train):len(df_train) + len(df_eval)] = 1\n",
    "test_mask = torch.zeros(len(df_all), dtype=int)\n",
    "test_mask[len(df_train) + len(df_eval):] = 1\n",
    "\n",
    "train_graph = dgl.edge_subgraph(graph2, train_mask.bool(), relabel_nodes=False)\n",
    "val_graph = dgl.edge_subgraph(graph2, val_mask.bool(), relabel_nodes=False)\n",
    "test_graph = dgl.edge_subgraph(graph2, test_mask.bool(), relabel_nodes=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1771, 1823, 1755,  ..., 1904,  377,  145]) torch.int64\n",
      "tensor([ 1,  2,  3,  ..., 57, 46, 44]) torch.int64\n",
      "tensor([ 533,  135, 1663,  ..., 1020, 1963, 1587]) torch.int64\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[270], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m TransE(graph2\u001b[38;5;241m.\u001b[39mnum_nodes(), \u001b[38;5;28mlen\u001b[39m(rel2id_graph), embedding_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m      2\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m13\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m auc \u001b[38;5;241m=\u001b[39m evaluate(model, test_graph)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mauc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[266], line 110\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, graph, optimizer, epochs)\u001b[0m\n\u001b[1;32m    108\u001b[0m h, t \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39medges()\n\u001b[1;32m    109\u001b[0m r \u001b[38;5;241m=\u001b[39m graph\u001b[38;5;241m.\u001b[39medata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 110\u001b[0m h_emb, r_emb, t_emb \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    111\u001b[0m neg_h, neg_t \u001b[38;5;241m=\u001b[39m generate_negative_samples(graph, h_emb\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mprint\u001b[39m(h_emb\u001b[38;5;241m.\u001b[39mshape, r_emb\u001b[38;5;241m.\u001b[39mshape, neg_h\u001b[38;5;241m.\u001b[39mshape, neg_t\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/miniconda3/envs/prime/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[266], line 75\u001b[0m, in \u001b[0;36mTransE.forward\u001b[0;34m(self, h, r, t)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(t,t\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m     74\u001b[0m h_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentity_embeddings(h)\n\u001b[0;32m---> 75\u001b[0m r_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelation_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m t_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentity_embeddings(t)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m h_emb, r_emb, t_emb\n",
      "File \u001b[0;32m~/miniconda3/envs/prime/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/prime/lib/python3.9/site-packages/torch/nn/modules/sparse.py:162\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/prime/lib/python3.9/site-packages/torch/nn/functional.py:2210\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2204\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2205\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2206\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2208\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2209\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "model = TransE(graph2.num_nodes(), len(rel2id_graph), embedding_dim=50)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "train(model, train_graph, optimizer, epochs=13)\n",
    "auc = evaluate(model, test_graph)\n",
    "print(f\"Test AUC: {auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'OneHotEncoder' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 54\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt understand \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m y_probas\n\u001b[0;32m---> 54\u001b[0m clf \u001b[38;5;241m=\u001b[39m \u001b[43mNEQBoost\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmin_residual_selection\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \n\u001b[1;32m     55\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X,y)\n\u001b[1;32m     56\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X)\n",
      "Cell \u001b[0;32mIn[16], line 7\u001b[0m, in \u001b[0;36mNEQBoost.__init__\u001b[0;34m(self, strategy)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_residual_selection\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mohe \u001b[38;5;241m=\u001b[39m \u001b[43mOneHotEncoder\u001b[49m()\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msc \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstrategy \u001b[38;5;241m=\u001b[39m strategy\n",
      "\u001b[0;31mNameError\u001b[0m: name 'OneHotEncoder' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from scipy.special import softmax\n",
    "\n",
    "class NEQBoost(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, strategy = 'min_residual_selection'):\n",
    "        self.ohe = OneHotEncoder()\n",
    "        self.sc = StandardScaler()\n",
    "        self.strategy = strategy\n",
    "        self.W = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X_sc = self.sc.fit_transform(X)\n",
    "        y_ohe = self.ohe.fit_transform(y.reshape(-1,1)).toarray()\n",
    "        wrong = np.arange(X_sc.shape[0])\n",
    "        while sum(wrong) > 0:\n",
    "            X_leftover, y_leftover = X_sc[wrong], y_ohe[wrong]\n",
    "            W_leftover, _, _, _ = np.linalg.lstsq(X_leftover, y_leftover, rcond=None)\n",
    "            preds_ohe = X_leftover@W_leftover\n",
    "            wrong = preds_ohe.argmax(axis=1) != y_leftover.argmax(axis=1)    \n",
    "            self.W.append(W_leftover)\n",
    "        self.num_neqs = len(self.W)\n",
    "        # print(f\"Rounds of boosting: {self.num_neqs}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return probas.argmax(axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_sc = self.sc.transform(X)\n",
    "        y_probas_all = []\n",
    "        dists = []\n",
    "        for W in self.W:\n",
    "            y_logit = X_sc @ W\n",
    "            dist = np.linalg.norm(y_logit, axis=1)\n",
    "            y_proba = softmax(y_logit, axis=1)\n",
    "            y_probas_all.append(y_proba)\n",
    "            dists.append(dist)\n",
    "        dists = np.vstack(dists).T\n",
    "        dists = softmax(dists, axis=1)\n",
    "        y_probas_all = np.array(y_probas_all).reshape(self.num_neqs, X.shape[0], -1)\n",
    "        #print(y_probas_all.shape)\n",
    "        if self.strategy == 'weighted_residual_voting':\n",
    "            y_probas = np.einsum('mic, im-> ic', y_probas_all, dists)\n",
    "        elif self.strategy == 'min_residual_selection':\n",
    "            neq_to_use = dists.argmin(axis=1)\n",
    "            y_probas = y_probas_all[neq_to_use, np.arange(X.shape[0]), :]\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Can't understand {self.strategy}\")\n",
    "        return y_probas\n",
    "\n",
    "clf = NEQBoost(strategy='min_residual_selection')  \n",
    "clf.fit(X,y)\n",
    "y_pred = clf.predict(X)\n",
    "print(f\"Full data fit acc: {accuracy_score(y, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def fit_subsamples(X_leftovers, labels_leftovers, max_to_check='sqrt'):\n",
    "    if max_to_check == 'sqrt':\n",
    "        max_num_clusters =  int(np.round(np.sqrt(X_leftovers.shape[0]),0)) + 1\n",
    "    elif max_to_check == 'full':\n",
    "        max_num_clusters =  X_leftovers.shape[0] + 1\n",
    "    elif isinstance(max_to_check, int):\n",
    "        max_num_clusters = max_to_check\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    res = []\n",
    "    cluster_dict = {}\n",
    "    for k in range(1, max_num_clusters):\n",
    "        cl = KMeans(n_clusters=k, n_init='auto')\n",
    "        cl.fit(X_leftovers)\n",
    "        cluster_dict[k] = {'cl':cl, 'W': []}\n",
    "        rr = 0\n",
    "        acc = 0\n",
    "        for unq_label in np.unique(cl.labels_):\n",
    "            current_cluster = np.where(cl.labels_ == unq_label)[0]\n",
    "            X_cur, y_cur_ohe = X_leftovers[current_cluster], labels_leftovers[current_cluster]\n",
    "            W_ohe_cur, residual_sums, _, _= np.linalg.lstsq(X_cur, y_cur_ohe, rcond=None)\n",
    "            cluster_dict[k]['W'].append(W_ohe_cur)\n",
    "            cur_acc = accuracy_score(y_cur_ohe.argmax(axis=1), (X_cur @ W_ohe_cur).argmax(axis=1))\n",
    "            rr += residual_sums.sum()\n",
    "            acc += cur_acc\n",
    "            \n",
    "        #print(rr)\n",
    "        res.append((int(k), cl.inertia_, rr, acc/k))\n",
    "        #break\n",
    "    res_df = pd.DataFrame(res, columns=['k', 'inertia', 'ss', 'acc'])\n",
    "    wanted_k = res_df.sort_values(['acc', 'k'], ascending=[False,True]).iloc[0]['k']\n",
    "    return cluster_dict[wanted_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data fit acc: 0.9772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from scipy.special import softmax\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "class NEQ_Local(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, strategy = 'selection', k=\"sqrt\"):\n",
    "        self.ohe = OneHotEncoder()\n",
    "        self.sc = StandardScaler()\n",
    "        self.k = k\n",
    "        if isinstance(self.k, int):\n",
    "            self.num_neigh = self.k\n",
    "        self.strategy = strategy\n",
    "        self.W = None\n",
    "        self.subsamples_dict = {}\n",
    "        self.neighbor_tree = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X_sc = self.sc.fit_transform(X)\n",
    "        \n",
    "        if self.k == 'auto':\n",
    "            accs = []\n",
    "            for k in range(1, int(np.sqrt(X.shape[0]))):\n",
    "                kn = KNeighborsClassifier(n_neighbors=k,)\n",
    "                kn.fit(X_sc, y)\n",
    "                accs.append(kn.score(X_sc, y))\n",
    "            self.num_neigh = np.argmax(accs) + 1\n",
    "                \n",
    "        elif self.k == 'sqrt':\n",
    "            self.num_neigh = int(np.sqrt(X.shape[0]))\n",
    "        \n",
    "        self.num_neigh = self.num_neigh + 1 if self.num_neigh % 2 == 0 else self.num_neigh\n",
    "            \n",
    "        \n",
    "        self.neighbor_tree = BallTree(X_sc)\n",
    "        \n",
    "        \n",
    "        y_ohe = self.ohe.fit_transform(y.reshape(-1,1)).toarray()\n",
    "        \n",
    "        self.W, _, _, _= np.linalg.lstsq(X_sc, y_ohe, rcond=None)\n",
    "        preds_ohe = X_sc@self.W\n",
    "        correct = (preds_ohe.argmax(axis=1) == y).astype(int)\n",
    "        self.neq_train_labels = correct\n",
    "        X_leftovers = X_sc[~correct.astype(bool)]\n",
    "        labels_leftovers = y_ohe[~correct.astype(bool)]\n",
    "        self.subsamples_dict = fit_subsamples(X_leftovers, labels_leftovers)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return probas.argmax(axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_sc = self.sc.transform(X)\n",
    "        dist, indices = self.neighbor_tree.query(X_sc, k=self.num_neigh)\n",
    "        # num_test X 1\n",
    "        neq_percentage_correct = self.neq_train_labels[indices].mean(axis=1)\n",
    "        y_neq = X_sc @ self.W\n",
    "        \n",
    "        if self.strategy == 'selection':\n",
    "            keep_neq = (neq_percentage_correct > 0.5)\n",
    "            if (~keep_neq).sum():\n",
    "                X_leftovers = X_sc[~keep_neq]\n",
    "                local_neq_to_use = self.subsamples_dict['cl'].predict(X_leftovers)\n",
    "                y_locals = []\n",
    "                for sample_index, neq_index in enumerate(local_neq_to_use):\n",
    "                    cur_X = X_leftovers[sample_index,:]\n",
    "                    cur_W = self.subsamples_dict['W'][neq_index]\n",
    "                    y_locals.append(cur_X @ cur_W)\n",
    "                y_locals = np.vstack(y_locals)\n",
    "                y_neq[~keep_neq] = y_locals\n",
    "            y_probas = softmax(y_neq, axis=1)\n",
    "        elif self.strategy == 'weighted_voting':\n",
    "            keep_neq = (neq_percentage_correct > 0.5)\n",
    "            if (~keep_neq).sum():\n",
    "                X_leftovers = X_sc[~keep_neq]\n",
    "                perc_nec = neq_percentage_correct[~keep_neq]\n",
    "                local_neq_to_use = self.subsamples_dict['cl'].predict(X_leftovers)\n",
    "                y_locals = []\n",
    "                for sample_index, neq_index in enumerate(local_neq_to_use):\n",
    "                    cur_X = X_leftovers[sample_index,:]\n",
    "                    cur_W = self.subsamples_dict['W'][neq_index]\n",
    "                    y_locals.append(cur_X @ cur_W)\n",
    "                y_locals = np.vstack(y_locals)\n",
    "                y_neq[~keep_neq] = perc_nec.reshape(-1,1) * softmax(y_neq[~keep_neq], axis=1) + (1-perc_nec).reshape(-1,1) * softmax(y_locals, axis=1)\n",
    "            y_probas = softmax(y_neq, axis=1)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Can't understand {self.strategy}\")\n",
    "    \n",
    "        return y_probas\n",
    "\n",
    "clf = NEQ_Local(k=3, strategy='weighted_voting')  \n",
    "clf.fit(X,y)\n",
    "y_pred = clf.predict(X)\n",
    "print(f\"Full data fit acc: {accuracy_score(y, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85       100\n",
      "           1       0.85      0.84      0.84       100\n",
      "\n",
      "    accuracy                           0.84       200\n",
      "   macro avg       0.85      0.84      0.84       200\n",
      "weighted avg       0.85      0.84      0.84       200\n",
      "\n",
      "[[85 15]\n",
      " [16 84]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42, max_depth=None)\n",
    "# y_pred = cross_val_predict(clf, X, y, cv=cv)\n",
    "\n",
    "y_pred_all = []\n",
    "y_true_all = []\n",
    "for train, test in cv.split(X,y):\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]\n",
    "    clf = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_all.extend(y_pred.tolist())\n",
    "    y_true_all.extend(y_test.tolist())\n",
    "\n",
    "print(classification_report(y_true_all, y_pred_all))\n",
    "print(confusion_matrix(y_true_all, y_pred_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      model      rank\n",
      "4     NEQ_Local_selection_5  3.864865\n",
      "3                       NEQ  3.905405\n",
      "2                        DT  3.986486\n",
      "6  NEQ_Local_selection_sqrt  4.297297\n",
      "7   NEQ_Local_weighted_auto  4.310811\n",
      "8   NEQ_Local_weighted_sqrt  4.554054\n",
      "5  NEQ_Local_selection_auto  4.675676\n",
      "1                Boost_Mean  7.486486\n",
      "0                 Boost_Max  7.918919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_867173/1341394748.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = res.groupby('dataset').apply(lambda x: x.sort_values(by='f1', ascending=False)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "res = pd.read_csv(\"./results/neq_boost_results.csv\")\n",
    "# Step 2: Sort each group by 'f1'\n",
    "sorted_df = res.groupby('dataset').apply(lambda x: x.sort_values(by='f1', ascending=False)).reset_index(drop=True)\n",
    "\n",
    "# Step 3: Assign ranks within each group\n",
    "sorted_df['rank'] = sorted_df.groupby('dataset').cumcount() + 1\n",
    "\n",
    "# Step 4: Calculate mean rank for each model across all datasets\n",
    "mean_ranks = sorted_df.groupby('model')['rank'].mean().reset_index().sort_values(by='rank')\n",
    "\n",
    "print(mean_ranks)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WINS\n",
      "                         DT  NEQ_Local_selection_5   NEQ\n",
      "DT                      0.0                   40.0  37.0\n",
      "NEQ_Local_selection_5  33.0                    0.0  41.0\n",
      "NEQ                    36.0                   25.0   0.0\n"
     ]
    }
   ],
   "source": [
    "models = mean_ranks.model[:3].values\n",
    "wins_score = np.zeros((len(models), len(models)))\n",
    "\n",
    "metric_to_score = 'f1'\n",
    "res_local = res[res['model'].isin(models)]\n",
    "for classification_dataset in res_local['dataset'].unique():\n",
    "    cur_df = res_local[res_local['dataset'] == classification_dataset]\n",
    "    cur_df = cur_df.set_index('model')\n",
    "    score_metric = cur_df[metric_to_score]\n",
    "    for i, m1 in enumerate(models):\n",
    "        for j, m2 in enumerate(models[i:]):\n",
    "            if cur_df.loc[m1][metric_to_score] > cur_df.loc[m2][metric_to_score]:\n",
    "                wins_score[i, j+i] += 1\n",
    "            elif cur_df.loc[m1][metric_to_score] < cur_df.loc[m2][metric_to_score]:\n",
    "                wins_score[j+i, i] += 1\n",
    "            else:\n",
    "                pass\n",
    "order_of_models = wins_score.mean(axis=1).argsort()[::-1]\n",
    "wins_score = wins_score[order_of_models, :][:, order_of_models]\n",
    "print('WINS')\n",
    "print(pd.DataFrame(wins_score, columns = np.array(models)[order_of_models], index=np.array(models)[order_of_models]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
