{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Local / Boosted Normal Equation models for classification\n",
    "### Date: 18/7/2024\n",
    "### Status: Works as a proof of concept across many datasets.\n",
    "### Idea: (on binary classification)\n",
    "The idea was to fit iteratively normal equations (analytically) on subsets of features in two ways:\n",
    "1. **Boosting** way:\n",
    "   1. Begin with the whole train dataset as wrongly classified.\n",
    "   2. Fit a NEQ on the whole dataset. The leftovers of this model, are used to fit the next NEQ and so forth, until no errors are made.\n",
    "   3. At inference time, use all the NEQ weight matrices to predict all the test samples.\n",
    "   4. The final prediction is based on the used *strategy*:\n",
    "      1. For \"min_residual_selection\", use the NEQ with the least residual loss for each sample.\n",
    "      2. For \"weighted_residual_voting\", use the mean prediction of the NEQs.\n",
    "2. **Local** way:\n",
    "   1. Again, start by fitting a NEQ on the data.\n",
    "   2. The leftovers are then grouped into a number of clusters, and k NEQ are fitted on them.\n",
    "   3. At inference time:\n",
    "      1. For each sample, we use the main NEQ to generate the prediction and also fetch the k most similar train samples and whether the NEQ predicted correct each of those k samples.\n",
    "      2. If, the NEQ performed > 50% on accuracy on the \"close\" train samples, the prediction is left as is.\n",
    "      3. For the cases, where the NEQ under-performed, we change the prediction based on the *strategy*:\n",
    "         1. For \"selection\" strategy, find the corresponding cluster/NEQ and use that to predict the final values.\n",
    "         2. For \"weighted_voting\" strategy, use a  weighted aggregation of the \"selection\" prediction and the original NEQ prediction, weighted by the percentage of incorrect predictions of the origial NEQ on the local query neighborhood.\n",
    "\n",
    "\n",
    "Some points:\n",
    "1. All models have standard scaling for NEQ.\n",
    "2. All models have OHE for classification. Thus the NEQ is essentially a multiple LSQ solution.\n",
    "3. For the **Local** variant the number of clusters is selected by keeping the np.sqrt(num_leftovers_from_NEQ)\n",
    "4. For the **Local** variant the number of **k** train samples to check the local goodness of fit, is selected by fitting a kNeighborsClassifier on the train for multiple k, and keeping the best performing one.\n",
    "\n",
    "\n",
    "### Results:\n",
    "1. This seems to work. Across 74 datasets vs DT, the mean rank of the NEQ Local models is better. Head to head DT is a bit better.\n",
    "2. From the variants of Local the, selection strategy seems better and also fixing the number of neighbors (rather than relying on kNeighbors tuning).\n",
    "3. NEQ Boost is worse on average. Strategy wise the min residual collection seems better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pmlb import fetch_data\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "y_ohe = OneHotEncoder().fit_transform(y.reshape(-1,1)).toarray()\n",
    "X_sc = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data fit acc: 0.9297\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from scipy.special import softmax\n",
    "\n",
    "class NEQBoost(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, strategy = 'min_residual_selection'):\n",
    "        self.ohe = OneHotEncoder()\n",
    "        self.sc = StandardScaler()\n",
    "        self.strategy = strategy\n",
    "        self.W = []\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X_sc = self.sc.fit_transform(X)\n",
    "        y_ohe = self.ohe.fit_transform(y.reshape(-1,1)).toarray()\n",
    "        wrong = np.arange(X_sc.shape[0])\n",
    "        while sum(wrong) > 0:\n",
    "            X_leftover, y_leftover = X_sc[wrong], y_ohe[wrong]\n",
    "            W_leftover, _, _, _ = np.linalg.lstsq(X_leftover, y_leftover, rcond=None)\n",
    "            preds_ohe = X_leftover@W_leftover\n",
    "            wrong = preds_ohe.argmax(axis=1) != y_leftover.argmax(axis=1)    \n",
    "            self.W.append(W_leftover)\n",
    "        self.num_neqs = len(self.W)\n",
    "        # print(f\"Rounds of boosting: {self.num_neqs}\")\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return probas.argmax(axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_sc = self.sc.transform(X)\n",
    "        y_probas_all = []\n",
    "        dists = []\n",
    "        for W in self.W:\n",
    "            y_logit = X_sc @ W\n",
    "            dist = np.linalg.norm(y_logit, axis=1)\n",
    "            y_proba = softmax(y_logit, axis=1)\n",
    "            y_probas_all.append(y_proba)\n",
    "            dists.append(dist)\n",
    "        dists = np.vstack(dists).T\n",
    "        dists = softmax(dists, axis=1)\n",
    "        y_probas_all = np.array(y_probas_all).reshape(self.num_neqs, X.shape[0], -1)\n",
    "        #print(y_probas_all.shape)\n",
    "        if self.strategy == 'weighted_residual_voting':\n",
    "            y_probas = np.einsum('mic, im-> ic', y_probas_all, dists)\n",
    "        elif self.strategy == 'min_residual_selection':\n",
    "            neq_to_use = dists.argmin(axis=1)\n",
    "            y_probas = y_probas_all[neq_to_use, np.arange(X.shape[0]), :]\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Can't understand {self.strategy}\")\n",
    "        return y_probas\n",
    "\n",
    "clf = NEQBoost(strategy='min_residual_selection')  \n",
    "clf.fit(X,y)\n",
    "y_pred = clf.predict(X)\n",
    "print(f\"Full data fit acc: {accuracy_score(y, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def fit_subsamples(X_leftovers, labels_leftovers, max_to_check='sqrt'):\n",
    "    if max_to_check == 'sqrt':\n",
    "        max_num_clusters =  int(np.round(np.sqrt(X_leftovers.shape[0]),0)) + 1\n",
    "    elif max_to_check == 'full':\n",
    "        max_num_clusters =  X_leftovers.shape[0] + 1\n",
    "    elif isinstance(max_to_check, int):\n",
    "        max_num_clusters = max_to_check\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "    res = []\n",
    "    cluster_dict = {}\n",
    "    for k in range(1, max_num_clusters):\n",
    "        cl = KMeans(n_clusters=k, n_init='auto')\n",
    "        cl.fit(X_leftovers)\n",
    "        cluster_dict[k] = {'cl':cl, 'W': []}\n",
    "        rr = 0\n",
    "        acc = 0\n",
    "        for unq_label in np.unique(cl.labels_):\n",
    "            current_cluster = np.where(cl.labels_ == unq_label)[0]\n",
    "            X_cur, y_cur_ohe = X_leftovers[current_cluster], labels_leftovers[current_cluster]\n",
    "            W_ohe_cur, residual_sums, _, _= np.linalg.lstsq(X_cur, y_cur_ohe, rcond=None)\n",
    "            cluster_dict[k]['W'].append(W_ohe_cur)\n",
    "            cur_acc = accuracy_score(y_cur_ohe.argmax(axis=1), (X_cur @ W_ohe_cur).argmax(axis=1))\n",
    "            rr += residual_sums.sum()\n",
    "            acc += cur_acc\n",
    "            \n",
    "        #print(rr)\n",
    "        res.append((int(k), cl.inertia_, rr, acc/k))\n",
    "        #break\n",
    "    res_df = pd.DataFrame(res, columns=['k', 'inertia', 'ss', 'acc'])\n",
    "    wanted_k = res_df.sort_values(['acc', 'k'], ascending=[False,True]).iloc[0]['k']\n",
    "    return cluster_dict[wanted_k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data fit acc: 0.9772\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from scipy.special import softmax\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "class NEQ_Local(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, strategy = 'selection', k=\"sqrt\"):\n",
    "        self.ohe = OneHotEncoder()\n",
    "        self.sc = StandardScaler()\n",
    "        self.k = k\n",
    "        if isinstance(self.k, int):\n",
    "            self.num_neigh = self.k\n",
    "        self.strategy = strategy\n",
    "        self.W = None\n",
    "        self.subsamples_dict = {}\n",
    "        self.neighbor_tree = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \n",
    "        X_sc = self.sc.fit_transform(X)\n",
    "        \n",
    "        if self.k == 'auto':\n",
    "            accs = []\n",
    "            for k in range(1, int(np.sqrt(X.shape[0]))):\n",
    "                kn = KNeighborsClassifier(n_neighbors=k,)\n",
    "                kn.fit(X_sc, y)\n",
    "                accs.append(kn.score(X_sc, y))\n",
    "            self.num_neigh = np.argmax(accs) + 1\n",
    "                \n",
    "        elif self.k == 'sqrt':\n",
    "            self.num_neigh = int(np.sqrt(X.shape[0]))\n",
    "        \n",
    "        self.num_neigh = self.num_neigh + 1 if self.num_neigh % 2 == 0 else self.num_neigh\n",
    "            \n",
    "        \n",
    "        self.neighbor_tree = BallTree(X_sc)\n",
    "        \n",
    "        \n",
    "        y_ohe = self.ohe.fit_transform(y.reshape(-1,1)).toarray()\n",
    "        \n",
    "        self.W, _, _, _= np.linalg.lstsq(X_sc, y_ohe, rcond=None)\n",
    "        preds_ohe = X_sc@self.W\n",
    "        correct = (preds_ohe.argmax(axis=1) == y).astype(int)\n",
    "        self.neq_train_labels = correct\n",
    "        X_leftovers = X_sc[~correct.astype(bool)]\n",
    "        labels_leftovers = y_ohe[~correct.astype(bool)]\n",
    "        self.subsamples_dict = fit_subsamples(X_leftovers, labels_leftovers)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        probas = self.predict_proba(X)\n",
    "        return probas.argmax(axis=1)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        X_sc = self.sc.transform(X)\n",
    "        dist, indices = self.neighbor_tree.query(X_sc, k=self.num_neigh)\n",
    "        # num_test X 1\n",
    "        neq_percentage_correct = self.neq_train_labels[indices].mean(axis=1)\n",
    "        y_neq = X_sc @ self.W\n",
    "        \n",
    "        if self.strategy == 'selection':\n",
    "            keep_neq = (neq_percentage_correct > 0.5)\n",
    "            if (~keep_neq).sum():\n",
    "                X_leftovers = X_sc[~keep_neq]\n",
    "                local_neq_to_use = self.subsamples_dict['cl'].predict(X_leftovers)\n",
    "                y_locals = []\n",
    "                for sample_index, neq_index in enumerate(local_neq_to_use):\n",
    "                    cur_X = X_leftovers[sample_index,:]\n",
    "                    cur_W = self.subsamples_dict['W'][neq_index]\n",
    "                    y_locals.append(cur_X @ cur_W)\n",
    "                y_locals = np.vstack(y_locals)\n",
    "                y_neq[~keep_neq] = y_locals\n",
    "            y_probas = softmax(y_neq, axis=1)\n",
    "        elif self.strategy == 'weighted_voting':\n",
    "            keep_neq = (neq_percentage_correct > 0.5)\n",
    "            if (~keep_neq).sum():\n",
    "                X_leftovers = X_sc[~keep_neq]\n",
    "                perc_nec = neq_percentage_correct[~keep_neq]\n",
    "                local_neq_to_use = self.subsamples_dict['cl'].predict(X_leftovers)\n",
    "                y_locals = []\n",
    "                for sample_index, neq_index in enumerate(local_neq_to_use):\n",
    "                    cur_X = X_leftovers[sample_index,:]\n",
    "                    cur_W = self.subsamples_dict['W'][neq_index]\n",
    "                    y_locals.append(cur_X @ cur_W)\n",
    "                y_locals = np.vstack(y_locals)\n",
    "                y_neq[~keep_neq] = perc_nec.reshape(-1,1) * softmax(y_neq[~keep_neq], axis=1) + (1-perc_nec).reshape(-1,1) * softmax(y_locals, axis=1)\n",
    "            y_probas = softmax(y_neq, axis=1)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Can't understand {self.strategy}\")\n",
    "    \n",
    "        return y_probas\n",
    "\n",
    "clf = NEQ_Local(k=3, strategy='weighted_voting')  \n",
    "clf.fit(X,y)\n",
    "y_pred = clf.predict(X)\n",
    "print(f\"Full data fit acc: {accuracy_score(y, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.85      0.85       100\n",
      "           1       0.85      0.84      0.84       100\n",
      "\n",
      "    accuracy                           0.84       200\n",
      "   macro avg       0.85      0.84      0.84       200\n",
      "weighted avg       0.85      0.84      0.84       200\n",
      "\n",
      "[[85 15]\n",
      " [16 84]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_predict, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42, max_depth=None)\n",
    "# y_pred = cross_val_predict(clf, X, y, cv=cv)\n",
    "\n",
    "y_pred_all = []\n",
    "y_true_all = []\n",
    "for train, test in cv.split(X,y):\n",
    "    X_train, y_train = X[train], y[train]\n",
    "    X_test, y_test = X[test], y[test]\n",
    "    clf = DecisionTreeClassifier(random_state=42, max_depth=3)\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    y_pred_all.extend(y_pred.tolist())\n",
    "    y_true_all.extend(y_test.tolist())\n",
    "\n",
    "print(classification_report(y_true_all, y_pred_all))\n",
    "print(confusion_matrix(y_true_all, y_pred_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      model      rank\n",
      "4     NEQ_Local_selection_5  3.864865\n",
      "3                       NEQ  3.905405\n",
      "2                        DT  3.986486\n",
      "6  NEQ_Local_selection_sqrt  4.297297\n",
      "7   NEQ_Local_weighted_auto  4.310811\n",
      "8   NEQ_Local_weighted_sqrt  4.554054\n",
      "5  NEQ_Local_selection_auto  4.675676\n",
      "1                Boost_Mean  7.486486\n",
      "0                 Boost_Max  7.918919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_867173/1341394748.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sorted_df = res.groupby('dataset').apply(lambda x: x.sort_values(by='f1', ascending=False)).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "res = pd.read_csv(\"./results/neq_boost_results.csv\")\n",
    "# Step 2: Sort each group by 'f1'\n",
    "sorted_df = res.groupby('dataset').apply(lambda x: x.sort_values(by='f1', ascending=False)).reset_index(drop=True)\n",
    "\n",
    "# Step 3: Assign ranks within each group\n",
    "sorted_df['rank'] = sorted_df.groupby('dataset').cumcount() + 1\n",
    "\n",
    "# Step 4: Calculate mean rank for each model across all datasets\n",
    "mean_ranks = sorted_df.groupby('model')['rank'].mean().reset_index().sort_values(by='rank')\n",
    "\n",
    "print(mean_ranks)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WINS\n",
      "                         DT  NEQ_Local_selection_5   NEQ\n",
      "DT                      0.0                   40.0  37.0\n",
      "NEQ_Local_selection_5  33.0                    0.0  41.0\n",
      "NEQ                    36.0                   25.0   0.0\n"
     ]
    }
   ],
   "source": [
    "models = mean_ranks.model[:3].values\n",
    "wins_score = np.zeros((len(models), len(models)))\n",
    "\n",
    "metric_to_score = 'f1'\n",
    "res_local = res[res['model'].isin(models)]\n",
    "for classification_dataset in res_local['dataset'].unique():\n",
    "    cur_df = res_local[res_local['dataset'] == classification_dataset]\n",
    "    cur_df = cur_df.set_index('model')\n",
    "    score_metric = cur_df[metric_to_score]\n",
    "    for i, m1 in enumerate(models):\n",
    "        for j, m2 in enumerate(models[i:]):\n",
    "            if cur_df.loc[m1][metric_to_score] > cur_df.loc[m2][metric_to_score]:\n",
    "                wins_score[i, j+i] += 1\n",
    "            elif cur_df.loc[m1][metric_to_score] < cur_df.loc[m2][metric_to_score]:\n",
    "                wins_score[j+i, i] += 1\n",
    "            else:\n",
    "                pass\n",
    "order_of_models = wins_score.mean(axis=1).argsort()[::-1]\n",
    "wins_score = wins_score[order_of_models, :][:, order_of_models]\n",
    "print('WINS')\n",
    "print(pd.DataFrame(wins_score, columns = np.array(models)[order_of_models], index=np.array(models)[order_of_models]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dgl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
