{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: Anomaly Detection using PCA on the features\n",
    "### Date: 18/7/2024\n",
    "### Status: In Progress.\n",
    "### Idea: \n",
    "Use PCA on X, transform back, highlight as anomalies the top-K with most distance from their reconstrunction?\n",
    "\n",
    "\n",
    "### Results:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7200, 6),\n",
       " 0    6666\n",
       " 1     534\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = np.load('./2_annthyroid.npz', allow_pickle=True)\n",
    "#data = np.load('./5_campaign(1).npz', allow_pickle=True)\n",
    "X, y = data['X'], data['y']\n",
    "X.shape, pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.94      0.95      1333\n",
      "           1       0.36      0.41      0.38       107\n",
      "\n",
      "    accuracy                           0.90      1440\n",
      "   macro avg       0.65      0.68      0.66      1440\n",
      "weighted avg       0.91      0.90      0.90      1440\n",
      "\n",
      "F1 pos: 0.3826\n",
      "AP: 0.3831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report,f1_score,average_precision_score\n",
    "clf = IsolationForest(random_state=42)\n",
    "clf.fit(X_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_pred_proba = -clf.decision_function(X_test)\n",
    "y_pred[y_pred == 1] = 0\n",
    "y_pred[y_pred == -1] = 1\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"F1 pos: {f1_score(y_test, y_pred, average='binary'):.4f}\")\n",
    "print(f\"AP: {average_precision_score(y_test, y_pred_proba):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kbougatiotis/miniconda3/envs/prime/lib/python3.9/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 1000 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>num_d</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>nmf</td>\n",
       "      <td>4</td>\n",
       "      <td>0.229508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>nmf</td>\n",
       "      <td>3</td>\n",
       "      <td>0.208000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>nmf</td>\n",
       "      <td>2</td>\n",
       "      <td>0.198473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>svd</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>pca</td>\n",
       "      <td>2</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pca</td>\n",
       "      <td>3</td>\n",
       "      <td>0.131148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>svd</td>\n",
       "      <td>3</td>\n",
       "      <td>0.131148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>svd</td>\n",
       "      <td>1</td>\n",
       "      <td>0.118644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pca</td>\n",
       "      <td>1</td>\n",
       "      <td>0.118644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>pca</td>\n",
       "      <td>6</td>\n",
       "      <td>0.114754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>pca</td>\n",
       "      <td>5</td>\n",
       "      <td>0.110236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>svd</td>\n",
       "      <td>5</td>\n",
       "      <td>0.110236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>nmf</td>\n",
       "      <td>1</td>\n",
       "      <td>0.051724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>svd</td>\n",
       "      <td>6</td>\n",
       "      <td>0.033333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>nmf</td>\n",
       "      <td>5</td>\n",
       "      <td>0.031496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>nmf</td>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pca</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>svd</td>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   model  num_d        f1\n",
       "9    nmf      4  0.229508\n",
       "8    nmf      3  0.208000\n",
       "7    nmf      2  0.198473\n",
       "13   svd      2  0.133333\n",
       "1    pca      2  0.133333\n",
       "2    pca      3  0.131148\n",
       "14   svd      3  0.131148\n",
       "12   svd      1  0.118644\n",
       "0    pca      1  0.118644\n",
       "5    pca      6  0.114754\n",
       "4    pca      5  0.110236\n",
       "16   svd      5  0.110236\n",
       "6    nmf      1  0.051724\n",
       "17   svd      6  0.033333\n",
       "10   nmf      5  0.031496\n",
       "11   nmf      6  0.000000\n",
       "3    pca      4  0.000000\n",
       "15   svd      4  0.000000"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler, RobustScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class ReconstructionAnomalyDetector(ClassifierMixin):\n",
    "    \"\"\"\n",
    "    Anomaly detection using reconstruction error from a matrix decomposition model.\n",
    "\n",
    "    This detector trains a matrix decomposition model (like PCA, SVD, or NMF)\n",
    "    on normal data. It then identifies anomalies by measuring the\n",
    "    reconstruction error of new data points. Points with a high reconstruction\n",
    "    error are considered anomalous.\n",
    "\n",
    "    Args:\n",
    "        model (str): The decomposition model to use ('pca', 'svd', or 'nmf').\n",
    "        n_components (int): The number of components for the decomposition model.\n",
    "        contamination (float or 'auto'): The expected proportion of anomalies.\n",
    "                           If 'auto', the threshold is determined automatically\n",
    "                           using the 3-sigma rule on the training errors.\n",
    "    \"\"\"\n",
    "    def __init__(self, model='pca', n_components=2, contamination=0.05):\n",
    "        if model not in ['pca', 'svd', 'nmf']:\n",
    "            raise ValueError(\"Model must be one of 'pca', 'svd', or 'nmf'\")\n",
    "        self.model_name = model\n",
    "        self.n_components = n_components\n",
    "        self.contamination = contamination\n",
    "        if model == 'nmf':\n",
    "            self.scaler = MaxAbsScaler()\n",
    "        else:\n",
    "            self.scaler = StandardScaler()\n",
    "        self.threshold_ = None\n",
    "\n",
    "        if self.model_name == 'pca':\n",
    "            self.model = PCA(n_components=self.n_components)\n",
    "        elif self.model_name == 'svd':\n",
    "            self.model = TruncatedSVD(n_components=self.n_components)\n",
    "        elif self.model_name == 'nmf':\n",
    "            self.model = NMF(n_components=self.n_components, init='random', random_state=0, max_iter=1000)\n",
    "\n",
    "    def _calculate_reconstruction_error(self, X):\n",
    "        \"\"\"Calculates the reconstruction error for each sample in X.\"\"\"\n",
    "        check_is_fitted(self.model)\n",
    "        X_transformed = self.model.transform(X)\n",
    "        X_reconstructed = self.model.inverse_transform(X_transformed)\n",
    "        return np.linalg.norm(X - X_reconstructed, axis=1)\n",
    "\n",
    "    def fit(self, X_train, y_train  = None):\n",
    "        \"\"\"\n",
    "        Fit the anomaly detector to the training data.\n",
    "\n",
    "        This involves training the decomposition model and setting the anomaly\n",
    "        threshold based on the reconstruction errors of the training data.\n",
    "\n",
    "        Args:\n",
    "            X_train (np.ndarray): The training data (samples x features).\n",
    "        \"\"\"\n",
    "        X_scaled = self.scaler.fit_transform(X_train)\n",
    "        self.model.fit(X_scaled)\n",
    "        \n",
    "        train_errors = self._calculate_reconstruction_error(X_scaled)\n",
    "        \n",
    "        if self.contamination == 'auto':\n",
    "            # Use the 3-sigma rule to set the threshold, a common statistical\n",
    "            # approach for outlier detection.\n",
    "            self.threshold_ = np.mean(train_errors) + 3 * np.std(train_errors)\n",
    "        else:\n",
    "            # Set the threshold based on the contamination parameter\n",
    "            self.threshold_ = np.quantile(train_errors, 1 - self.contamination)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict whether samples in X are anomalies.\n",
    "\n",
    "        Args:\n",
    "            X (np.ndarray): The data to predict on.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: An array of 0s (normal) and 1s (anomaly).\n",
    "        \"\"\"\n",
    "        if self.threshold_ is None:\n",
    "            raise RuntimeError(\"The model must be fitted before prediction.\")\n",
    "\n",
    "        \n",
    "        errors = self.decision_function(X)\n",
    "        # Return 1 for anomalies (error > threshold), 0 for normal\n",
    "        return (errors > self.threshold_).astype(int)\n",
    "    \n",
    "    def decision_function(self, X):\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        errors = self._calculate_reconstruction_error(X_scaled)\n",
    "        return errors\n",
    "    \n",
    "res = []\n",
    "for t in ['pca', 'nmf', 'svd']:\n",
    "    for n_comp in [1, 2,3,4,5, 6]:\n",
    "        clf = ReconstructionAnomalyDetector(model=t, n_components=n_comp, contamination='auto')\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        f1 = f1_score(y_test, y_pred, average='binary')\n",
    "        # print(classification_report(y_test, y_pred))\n",
    "        # print(f1)\n",
    "        res.append((t, n_comp, f1))\n",
    "res_df = pd.DataFrame(res, columns=['model', 'num_d', 'f1'])\n",
    "res_df = res_df.sort_values('f1', ascending=False)\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, NMF\n",
    "from sklearn.preprocessing import StandardScaler, MaxAbsScaler\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.utils.random import sample_without_replacement\n",
    "\n",
    "class PatchBasedAnomalyDetector:\n",
    "    \"\"\"\n",
    "    An ensemble anomaly detector that trains multiple PCA models on random\n",
    "    patches (subsets) of features. The final anomaly score is the average\n",
    "    reconstruction error across all models.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_patches=10, patch_size=0.5, n_components=2, contamination=0.05, model='pca'):\n",
    "        self.n_patches = n_patches\n",
    "        self.patch_size = patch_size\n",
    "        self.n_components = n_components\n",
    "        self.contamination = contamination\n",
    "        self.model = model\n",
    "        self.models = []\n",
    "        self.feature_indices = []\n",
    "        self.threshold_ = None\n",
    "\n",
    "    def fit(self, X_train):\n",
    "        n_samples, n_features = X_train.shape\n",
    "        \n",
    "        if isinstance(self.patch_size, float):\n",
    "            k = int(n_features * self.patch_size)\n",
    "        else:\n",
    "            k = self.patch_size\n",
    "        \n",
    "        if k < self.n_components:\n",
    "            raise ValueError(f\"patch_size ({k}) is smaller than n_components ({self.n_components}).\")\n",
    "\n",
    "        for _ in range(self.n_patches):\n",
    "            indices = sample_without_replacement(n_features, k, random_state=_) # for reproducibility\n",
    "            self.feature_indices.append(indices)\n",
    "\n",
    "            detector = ReconstructionAnomalyDetector(model=self.model, n_components=self.n_components, contamination=self.contamination)\n",
    "            detector.fit(X_train[:, indices])\n",
    "            self.models.append(detector)\n",
    "        \n",
    "        train_scores = self.decision_function(X_train)\n",
    "        \n",
    "        if self.contamination == 'auto':\n",
    "            # Use the 3-sigma rule to set the threshold, a common statistical\n",
    "            # approach for outlier detection.\n",
    "            self.threshold_ = np.mean(train_scores) + 2 * np.std(train_scores)\n",
    "        else:\n",
    "            # Set the threshold based on the contamination parameter\n",
    "            self.threshold_ = np.quantile(train_scores, 1 - self.contamination)\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        all_errors = np.zeros((X.shape[0], self.n_patches))\n",
    "        for i, (model, indices) in enumerate(zip(self.models, self.feature_indices)):\n",
    "            all_errors[:, i] = model.decision_function(X[:, indices])\n",
    "        return np.mean(all_errors, axis=1)\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = self.decision_function(X)\n",
    "        return (scores > self.threshold_).astype(int)\n",
    "\n",
    "\n",
    "class LocalPCADetector:\n",
    "    \"\"\"\n",
    "    Applies PCA locally by first clustering the data and then fitting a separate\n",
    "    PCA model to each cluster. Anomaly scores are calculated using the PCA model\n",
    "    of the closest cluster.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters=5, n_components=2, contamination=0.05, model='pca'):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.n_components = n_components\n",
    "        self.contamination = contamination\n",
    "        self.model = model\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init='auto')\n",
    "        self.pca_models = {}\n",
    "        self.threshold_ = None\n",
    "\n",
    "    def fit(self, X_train):\n",
    "        labels = self.kmeans.fit_predict(X_train)\n",
    "        \n",
    "        all_train_errors = []\n",
    "        for i in range(self.n_clusters):\n",
    "            cluster_data = X_train[labels == i]\n",
    "            if len(cluster_data) <= self.n_components:\n",
    "                # Not enough data in cluster, skip\n",
    "                continue\n",
    "            \n",
    "            detector = ReconstructionAnomalyDetector(model=self.model, n_components=self.n_components, contamination=self.contamination)\n",
    "            detector.fit(cluster_data)\n",
    "            self.pca_models[i] = detector\n",
    "            \n",
    "            # To set a global threshold, we calculate errors for all training points\n",
    "            # on their assigned cluster's model\n",
    "            cluster_errors = self.decision_function(cluster_data, precomputed_labels=np.full(len(cluster_data), i))\n",
    "            all_train_errors.append(cluster_errors)\n",
    "\n",
    "        if not all_train_errors:\n",
    "             raise RuntimeError(\"Could not fit any local PCA models. Try reducing n_clusters or providing more data.\")\n",
    "\n",
    "        all_train_errors = np.concatenate(all_train_errors)\n",
    "        if self.contamination == 'auto':\n",
    "            # Use the 3-sigma rule to set the threshold, a common statistical\n",
    "            # approach for outlier detection.\n",
    "            self.threshold_ = np.mean(all_train_errors) + 2 * np.std(all_train_errors)\n",
    "        else:\n",
    "            # Set the threshold based on the contamination parameter\n",
    "            self.threshold_ = np.quantile(all_train_errors, 1 - self.contamination)\n",
    "        return self\n",
    "\n",
    "    def decision_function(self, X, precomputed_labels=None):\n",
    "        if precomputed_labels is None:\n",
    "            labels = self.kmeans.predict(X)\n",
    "        else:\n",
    "            labels = precomputed_labels\n",
    "            \n",
    "        errors = np.zeros(X.shape[0])\n",
    "        for i in range(self.n_clusters):\n",
    "            if i in self.pca_models:\n",
    "                mask = (labels == i)\n",
    "                if np.any(mask):\n",
    "                    errors[mask] = self.pca_models[i].decision_function(X[mask])\n",
    "        return errors\n",
    "\n",
    "    def predict(self, X):\n",
    "        scores = self.decision_function(X)\n",
    "        return (scores > self.threshold_).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 3. Patch-Based PCA ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.94      0.99      0.96      1333\n",
      "     anomaly       0.55      0.17      0.26       107\n",
      "\n",
      "    accuracy                           0.93      1440\n",
      "   macro avg       0.74      0.58      0.61      1440\n",
      "weighted avg       0.91      0.93      0.91      1440\n",
      "\n",
      "--- 4. Local PCA (via Clustering) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.94      0.98      0.96      1333\n",
      "     anomaly       0.44      0.22      0.30       107\n",
      "\n",
      "    accuracy                           0.92      1440\n",
      "   macro avg       0.69      0.60      0.63      1440\n",
      "weighted avg       0.90      0.92      0.91      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 3. Using Patch-Based PCA ---\n",
    "print(\"--- 3. Patch-Based PCA ---\")\n",
    "detector_patch = PatchBasedAnomalyDetector(n_patches=10, patch_size=4, n_components=4, contamination='auto', model='svd')\n",
    "detector_patch.fit(X_train)\n",
    "predictions_patch = detector_patch.predict(X_test)\n",
    "print(classification_report(y_test, predictions_patch, target_names=['normal', 'anomaly']))\n",
    "\n",
    "# --- 4. Using Local PCA (via Clustering) ---\n",
    "print(\"--- 4. Local PCA (via Clustering) ---\")\n",
    "detector_local = LocalPCADetector(n_clusters=10, n_components=6, contamination='auto', model='svd')\n",
    "detector_local.fit(X_train)\n",
    "predictions_local = detector_local.predict(X_test)\n",
    "print(classification_report(y_test, predictions_local, target_names=['normal', 'anomaly']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prime",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
